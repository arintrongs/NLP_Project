{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import codecs\n",
    "import collections\n",
    "import keras.preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Embedding,TimeDistributed,Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import GRU, Dropout\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import Embedding,TimeDistributed,Flatten\n",
    "from keras.layers import BatchNormalization,Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pbs = pd.read_csv('../data/Thaipbs-tokenize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pbs.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# input_pbs.drop('Unnamed: 0.1', axis=1,inplace=True)\n",
    "# input_pbs.drop('Unnamed: 1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "      <th>DOW</th>\n",
       "      <th>time</th>\n",
       "      <th>view</th>\n",
       "      <th>category</th>\n",
       "      <th>tag</th>\n",
       "      <th>hour</th>\n",
       "      <th>numTag</th>\n",
       "      <th>token</th>\n",
       "      <th>numToken</th>\n",
       "      <th>numChar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ฝุ่น PM2.5 : ทส.เตรียมตั้งศูนย์แก้ปัญหาหมอกควั...</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:37</td>\n",
       "      <td>177</td>\n",
       "      <td>สิ่งแวดล้อม</td>\n",
       "      <td>ฝุ่นเชียงใหม่,ฝุ่นPM2.5,ฝุ่นคลุมเมือง,เชียงใหม...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>ฝุ่น,PM,ทส.,เตรียม,ศูนย์,แก้,ปัญหา,หมอก,ควัน,ข...</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>เลือกตั้ง 2562 : \"เพื่อไทย\" เตรียมยื่น กกต.จัด...</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:20</td>\n",
       "      <td>702</td>\n",
       "      <td>การเมือง</td>\n",
       "      <td>เลือกตั้ง62,เพื่อ่ไทย,กกต.,นับคะแนน,ไทยพีบีเอส...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>เลือกตั้ง,2562,ไทย,เตรียม,ยื่น,กกต.,เลือกตั้ง,...</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>เตรียมเอาผิด รพ.เอกชน 58 แห่งไม่ส่งข้อมูลราคายา</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:18</td>\n",
       "      <td>583</td>\n",
       "      <td>สาธารณสุข</td>\n",
       "      <td>ยา,เวชภัณฑ์ฯ,โรงพยาบาล,ค่าบริการทางการแพทย์,กร...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>เตรียม,รพ.เอกชน,ข้อมูล,ราคา,ยา</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ตั้งข้อสังเกตปลดล็อกกัญชา เอื้อนายทุนหรือไม่?</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:16</td>\n",
       "      <td>928</td>\n",
       "      <td>อาชญากรรม</td>\n",
       "      <td>กัญชา,มูลนิธิข้าวขวัญ,ThaiPBSnews</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>ข้อ,สังเกต,ปลด,ล็อก,กัญชา,เอื้อ,ทุน</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>คำพิพากษาเต็ม \"เปรมชัย\" หลุดคดีครอบครองซากเสือดำ</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:29</td>\n",
       "      <td>5163</td>\n",
       "      <td>สิ่งแวดล้อม</td>\n",
       "      <td>เสือดำ,เปรมชัย,เขตรักษาพันธุ์สัตว์ป่าทุ่งใหญ่น...</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>พิพากษา,เต็ม,เปรมชัย,หลุดคดี,ครอบครอง,ซาก,เสือดำ</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline        date  DOW   time  \\\n",
       "0  ฝุ่น PM2.5 : ทส.เตรียมตั้งศูนย์แก้ปัญหาหมอกควั...  05/04/2562  FRI  19:37   \n",
       "1  เลือกตั้ง 2562 : \"เพื่อไทย\" เตรียมยื่น กกต.จัด...  05/04/2562  FRI  19:20   \n",
       "2    เตรียมเอาผิด รพ.เอกชน 58 แห่งไม่ส่งข้อมูลราคายา  05/04/2562  FRI  19:18   \n",
       "3      ตั้งข้อสังเกตปลดล็อกกัญชา เอื้อนายทุนหรือไม่?  05/04/2562  FRI  19:16   \n",
       "4   คำพิพากษาเต็ม \"เปรมชัย\" หลุดคดีครอบครองซากเสือดำ  05/04/2562  FRI  19:29   \n",
       "\n",
       "   view     category                                                tag  hour  \\\n",
       "0   177  สิ่งแวดล้อม  ฝุ่นเชียงใหม่,ฝุ่นPM2.5,ฝุ่นคลุมเมือง,เชียงใหม...    19   \n",
       "1   702     การเมือง  เลือกตั้ง62,เพื่อ่ไทย,กกต.,นับคะแนน,ไทยพีบีเอส...    19   \n",
       "2   583    สาธารณสุข  ยา,เวชภัณฑ์ฯ,โรงพยาบาล,ค่าบริการทางการแพทย์,กร...    19   \n",
       "3   928    อาชญากรรม                  กัญชา,มูลนิธิข้าวขวัญ,ThaiPBSnews    19   \n",
       "4  5163  สิ่งแวดล้อม  เสือดำ,เปรมชัย,เขตรักษาพันธุ์สัตว์ป่าทุ่งใหญ่น...    19   \n",
       "\n",
       "   numTag                                              token  numToken  \\\n",
       "0      10  ฝุ่น,PM,ทส.,เตรียม,ศูนย์,แก้,ปัญหา,หมอก,ควัน,ข...        11   \n",
       "1       6  เลือกตั้ง,2562,ไทย,เตรียม,ยื่น,กกต.,เลือกตั้ง,...         9   \n",
       "2       9                     เตรียม,รพ.เอกชน,ข้อมูล,ราคา,ยา         5   \n",
       "3       3                ข้อ,สังเกต,ปลด,ล็อก,กัญชา,เอื้อ,ทุน         7   \n",
       "4       5   พิพากษา,เต็ม,เปรมชัย,หลุดคดี,ครอบครอง,ซาก,เสือดำ         7   \n",
       "\n",
       "   numChar  \n",
       "0       43  \n",
       "1       46  \n",
       "2       26  \n",
       "3       29  \n",
       "4       42  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQRval = input_pbs['view'].describe().loc['75%']-input_pbs['view'].describe().loc['25%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pbs['view'].describe().loc['25%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252.0 3499.5\n"
     ]
    }
   ],
   "source": [
    "outlierMin = max(input_pbs['view'].describe().loc['25%'],0)\n",
    "outlierMax = input_pbs['view'].describe().loc['75%']+1.5*IQRval\n",
    "print(outlierMin,outlierMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pbs = input_pbs[(input_pbs['view']<=outlierMax) & (input_pbs['view']>=outlierMin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17521.000000\n",
       "mean         8.386565\n",
       "std          2.509898\n",
       "min          1.000000\n",
       "25%          7.000000\n",
       "50%          8.000000\n",
       "75%         10.000000\n",
       "max         20.000000\n",
       "Name: numToken, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pbs['numToken'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17521.000000\n",
       "mean       994.658924\n",
       "std        772.189473\n",
       "min        252.000000\n",
       "25%        411.000000\n",
       "50%        697.000000\n",
       "75%       1337.000000\n",
       "max       3496.000000\n",
       "Name: view, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pbs['view'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3496"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAXVIEW = input_pbs['view'].max()\n",
    "MAXVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pbs_train, input_pbs_test = train_test_split(input_pbs, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText_train = []\n",
    "for sent in input_pbs_train['token']:\n",
    "    inputText_train.append(sent.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText_test = []\n",
    "for sent in input_pbs_test['token']:\n",
    "    inputText_test.append(sent.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLabel_train = []\n",
    "for view in input_pbs_train['view']:\n",
    "    inputLabel_train.append(view/MAXVIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLabel_test = []\n",
    "for view in input_pbs_test['view']:\n",
    "    inputLabel_test.append(view/MAXVIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14016 14016\n"
     ]
    }
   ],
   "source": [
    "print(len(inputText_train),len(inputLabel_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505 3505\n"
     ]
    }
   ],
   "source": [
    "print(len(inputText_test),len(inputLabel_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sentence in inputText_train:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "        \n",
    "word_count = list()\n",
    "word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "word_count = word_count[:len(word_count)//4]\n",
    "word_count.append((\"UNK\",0))\n",
    "\n",
    "train_word = set()\n",
    "for i in word_count:\n",
    "    train_word.add(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3514"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token = set()\n",
    "for word in train_word:\n",
    "    all_token.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token = list(all_token)\n",
    "all_token.insert(0,'for padding')\n",
    "all_token.insert(1,'UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3516"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_map = dict(zip(all_token, range(len(all_token))))\n",
    "token_map_reverse = dict(zip(range(len(all_token)),all_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train = np.asarray(input_pbs_train['token'].str.split(','))\n",
    "input_data_test = np.asarray(input_pbs_test['token'].str.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word(sent):\n",
    "    global all_token, token_map\n",
    "    result = np.zeros(len(all_token))\n",
    "    np_token = np.array(sent)\n",
    "    str_token, str_token_count = np.unique(np_token, return_counts=True)\n",
    "    for char, count in zip(str_token, str_token_count):\n",
    "        if char not in token_map.keys():\n",
    "            char = 'UNK'\n",
    "        result[token_map[char]] = count\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = np.vectorize(count_word, otypes=[object])(input_data_train)\n",
    "x_f1_train = np.array([[e for e in sl] for sl in temp_train.tolist()])\n",
    "temp_test = np.vectorize(count_word, otypes=[object])(input_data_test)\n",
    "x_f1_test = np.array([[e for e in sl] for sl in temp_test.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/burin/py_36_env/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/burin/py_36_env/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "trainContinuous = cs.fit_transform(input_pbs_train[['numToken','numChar','numTag']])\n",
    "testContinuous = cs.fit_transform(input_pbs_test[['numToken','numChar','numTag']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cetagorial feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipBinarizer = lb.fit(input_pbs[\"DOW\"])\n",
    "trainCategorical_dow = zipBinarizer.transform(input_pbs_train[\"DOW\"])\n",
    "testCategorical_dow = zipBinarizer.transform(input_pbs_test[\"DOW\"])\n",
    "zipBinarizer = lb.fit(input_pbs[\"hour\"])\n",
    "trainCategorical_hour = zipBinarizer.transform(input_pbs_train[\"hour\"])\n",
    "testCategorical_hour = zipBinarizer.transform(input_pbs_test[\"hour\"])\n",
    "zipBinarizer = lb.fit(input_pbs[\"category\"])\n",
    "trainCategorical_category = zipBinarizer.transform(input_pbs_train[\"category\"])\n",
    "testCategorical_category = zipBinarizer.transform(input_pbs_test[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.hstack([ trainContinuous,trainCategorical_dow,trainCategorical_hour,trainCategorical_category])\n",
    "x_test = np.hstack([ testContinuous,testCategorical_dow,testCategorical_hour,testCategorical_category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.hstack([x_f1_train,trainCategorical_dow,trainCategorical_hour,trainCategorical_category])\n",
    "# x_test = np.hstack([x_f1_test,testCategorical_dow,testCategorical_hour,testCategorical_category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.hstack([x_f1_train, trainContinuous])\n",
    "# x_test = np.hstack([x_f1_test, testContinuous])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14016, 51)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (14016, 3564)\n",
      "test size (3505, 3564)\n"
     ]
    }
   ],
   "source": [
    "print(\"train size\",x_train.shape)\n",
    "print(\"test size\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = inputLabel_train\n",
    "y_train = np.asarray(y_train).reshape(-1,1)\n",
    "y_test = inputLabel_test\n",
    "y_test = np.asarray(y_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test =  train_test_split(y_train, test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 14016\n",
      "test size 3505\n"
     ]
    }
   ],
   "source": [
    "print(\"train size\",len(y_train))\n",
    "print(\"test size\",len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():    \n",
    "    input1 = Input(shape=(x_train.shape[1],))\n",
    "    x = Dense(1024, activation='relu')(input1)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)\n",
    "    x = Dense(1,activation='linear')(x)\n",
    "    model = Model(inputs=input1, outputs=x)\n",
    "    adam  = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam,  loss='mean_absolute_percentage_error')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        (None, 3564)              0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 1024)              3650560   \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_113 (Bat (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_114 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,935,905\n",
      "Trainable params: 3,933,185\n",
      "Non-trainable params: 2,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path='./model_baseline_test3.h5'\n",
    "callbacks_list = [\n",
    "        ModelCheckpoint(\n",
    "            weight_path,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11212 samples, validate on 2804 samples\n",
      "Epoch 1/50\n",
      "11212/11212 [==============================] - 12s 1ms/step - loss: 491.3832 - val_loss: 231.5566\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 231.55657, saving model to ./model_baseline_test3.h5\n",
      "Epoch 2/50\n",
      "11212/11212 [==============================] - 7s 601us/step - loss: 402.9977 - val_loss: 194.0217\n",
      "\n",
      "Epoch 00002: val_loss improved from 231.55657 to 194.02172, saving model to ./model_baseline_test3.h5\n",
      "Epoch 3/50\n",
      "11212/11212 [==============================] - 7s 592us/step - loss: 347.9180 - val_loss: 157.2336\n",
      "\n",
      "Epoch 00003: val_loss improved from 194.02172 to 157.23362, saving model to ./model_baseline_test3.h5\n",
      "Epoch 4/50\n",
      "11212/11212 [==============================] - 7s 588us/step - loss: 302.4409 - val_loss: 132.8703\n",
      "\n",
      "Epoch 00004: val_loss improved from 157.23362 to 132.87033, saving model to ./model_baseline_test3.h5\n",
      "Epoch 5/50\n",
      "11212/11212 [==============================] - 7s 591us/step - loss: 264.7050 - val_loss: 118.1607\n",
      "\n",
      "Epoch 00005: val_loss improved from 132.87033 to 118.16072, saving model to ./model_baseline_test3.h5\n",
      "Epoch 6/50\n",
      "11212/11212 [==============================] - 7s 586us/step - loss: 235.1313 - val_loss: 105.5375\n",
      "\n",
      "Epoch 00006: val_loss improved from 118.16072 to 105.53753, saving model to ./model_baseline_test3.h5\n",
      "Epoch 7/50\n",
      "11212/11212 [==============================] - 7s 598us/step - loss: 205.6460 - val_loss: 90.2396\n",
      "\n",
      "Epoch 00007: val_loss improved from 105.53753 to 90.23961, saving model to ./model_baseline_test3.h5\n",
      "Epoch 8/50\n",
      "11212/11212 [==============================] - 7s 588us/step - loss: 179.4451 - val_loss: 82.1982\n",
      "\n",
      "Epoch 00008: val_loss improved from 90.23961 to 82.19824, saving model to ./model_baseline_test3.h5\n",
      "Epoch 9/50\n",
      "11212/11212 [==============================] - 7s 592us/step - loss: 162.5472 - val_loss: 74.9029\n",
      "\n",
      "Epoch 00009: val_loss improved from 82.19824 to 74.90292, saving model to ./model_baseline_test3.h5\n",
      "Epoch 10/50\n",
      "11212/11212 [==============================] - 7s 594us/step - loss: 143.4583 - val_loss: 68.9989\n",
      "\n",
      "Epoch 00010: val_loss improved from 74.90292 to 68.99892, saving model to ./model_baseline_test3.h5\n",
      "Epoch 11/50\n",
      "11212/11212 [==============================] - 7s 592us/step - loss: 128.3036 - val_loss: 64.4692\n",
      "\n",
      "Epoch 00011: val_loss improved from 68.99892 to 64.46924, saving model to ./model_baseline_test3.h5\n",
      "Epoch 12/50\n",
      "11212/11212 [==============================] - 7s 586us/step - loss: 113.0019 - val_loss: 59.5918\n",
      "\n",
      "Epoch 00012: val_loss improved from 64.46924 to 59.59183, saving model to ./model_baseline_test3.h5\n",
      "Epoch 13/50\n",
      "11212/11212 [==============================] - 7s 595us/step - loss: 100.3344 - val_loss: 56.5131\n",
      "\n",
      "Epoch 00013: val_loss improved from 59.59183 to 56.51309, saving model to ./model_baseline_test3.h5\n",
      "Epoch 14/50\n",
      "11212/11212 [==============================] - 7s 596us/step - loss: 90.0744 - val_loss: 53.6256\n",
      "\n",
      "Epoch 00014: val_loss improved from 56.51309 to 53.62563, saving model to ./model_baseline_test3.h5\n",
      "Epoch 15/50\n",
      "11212/11212 [==============================] - 7s 583us/step - loss: 79.4175 - val_loss: 51.6920\n",
      "\n",
      "Epoch 00015: val_loss improved from 53.62563 to 51.69204, saving model to ./model_baseline_test3.h5\n",
      "Epoch 16/50\n",
      "11212/11212 [==============================] - 7s 587us/step - loss: 71.6759 - val_loss: 49.7048\n",
      "\n",
      "Epoch 00016: val_loss improved from 51.69204 to 49.70477, saving model to ./model_baseline_test3.h5\n",
      "Epoch 17/50\n",
      "11212/11212 [==============================] - 7s 587us/step - loss: 65.9675 - val_loss: 48.7085\n",
      "\n",
      "Epoch 00017: val_loss improved from 49.70477 to 48.70850, saving model to ./model_baseline_test3.h5\n",
      "Epoch 18/50\n",
      "11212/11212 [==============================] - 6s 577us/step - loss: 60.7910 - val_loss: 47.9049\n",
      "\n",
      "Epoch 00018: val_loss improved from 48.70850 to 47.90489, saving model to ./model_baseline_test3.h5\n",
      "Epoch 19/50\n",
      "11212/11212 [==============================] - 6s 576us/step - loss: 55.9842 - val_loss: 47.6005\n",
      "\n",
      "Epoch 00019: val_loss improved from 47.90489 to 47.60046, saving model to ./model_baseline_test3.h5\n",
      "Epoch 20/50\n",
      "11212/11212 [==============================] - 7s 583us/step - loss: 53.8430 - val_loss: 47.3737\n",
      "\n",
      "Epoch 00020: val_loss improved from 47.60046 to 47.37368, saving model to ./model_baseline_test3.h5\n",
      "Epoch 21/50\n",
      "11212/11212 [==============================] - 6s 577us/step - loss: 51.6715 - val_loss: 47.6790\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 47.37368\n",
      "Epoch 22/50\n",
      "11212/11212 [==============================] - 7s 586us/step - loss: 50.0853 - val_loss: 47.0260\n",
      "\n",
      "Epoch 00022: val_loss improved from 47.37368 to 47.02600, saving model to ./model_baseline_test3.h5\n",
      "Epoch 23/50\n",
      "11212/11212 [==============================] - 6s 571us/step - loss: 49.0497 - val_loss: 47.2151\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 47.02600\n",
      "Epoch 24/50\n",
      "11212/11212 [==============================] - 6s 579us/step - loss: 48.3806 - val_loss: 47.0800\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 47.02600\n",
      "Epoch 25/50\n",
      "11212/11212 [==============================] - 6s 577us/step - loss: 47.4242 - val_loss: 47.2841\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 47.02600\n",
      "Epoch 26/50\n",
      "11212/11212 [==============================] - 7s 586us/step - loss: 47.0492 - val_loss: 46.8824\n",
      "\n",
      "Epoch 00026: val_loss improved from 47.02600 to 46.88241, saving model to ./model_baseline_test3.h5\n",
      "Epoch 27/50\n",
      "11212/11212 [==============================] - 7s 584us/step - loss: 46.5231 - val_loss: 47.0816\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 46.88241\n",
      "Epoch 28/50\n",
      "11212/11212 [==============================] - 6s 580us/step - loss: 46.0765 - val_loss: 46.8316\n",
      "\n",
      "Epoch 00028: val_loss improved from 46.88241 to 46.83159, saving model to ./model_baseline_test3.h5\n",
      "Epoch 29/50\n",
      "11212/11212 [==============================] - 6s 576us/step - loss: 45.6752 - val_loss: 46.8258\n",
      "\n",
      "Epoch 00029: val_loss improved from 46.83159 to 46.82584, saving model to ./model_baseline_test3.h5\n",
      "Epoch 30/50\n",
      "11212/11212 [==============================] - 7s 588us/step - loss: 45.0021 - val_loss: 46.7117\n",
      "\n",
      "Epoch 00030: val_loss improved from 46.82584 to 46.71174, saving model to ./model_baseline_test3.h5\n",
      "Epoch 31/50\n",
      "11212/11212 [==============================] - 7s 586us/step - loss: 44.2310 - val_loss: 46.8169\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 46.71174\n",
      "Epoch 32/50\n",
      "11212/11212 [==============================] - 6s 580us/step - loss: 43.3722 - val_loss: 46.8341\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 46.71174\n",
      "Epoch 33/50\n",
      "11212/11212 [==============================] - 7s 580us/step - loss: 42.9425 - val_loss: 46.8680\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 46.71174\n",
      "Epoch 34/50\n",
      "11212/11212 [==============================] - 7s 588us/step - loss: 41.5933 - val_loss: 47.1218\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 46.71174\n",
      "Epoch 35/50\n",
      "11212/11212 [==============================] - 7s 585us/step - loss: 40.8246 - val_loss: 47.2882\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 46.71174\n",
      "Epoch 36/50\n",
      "11212/11212 [==============================] - 7s 583us/step - loss: 40.0089 - val_loss: 47.5825\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 46.71174\n",
      "Epoch 37/50\n",
      "11212/11212 [==============================] - 7s 587us/step - loss: 39.0269 - val_loss: 48.1426\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 46.71174\n",
      "Epoch 38/50\n",
      "11212/11212 [==============================] - 7s 587us/step - loss: 37.8689 - val_loss: 48.5735\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 46.71174\n",
      "Epoch 39/50\n",
      "11212/11212 [==============================] - 7s 583us/step - loss: 37.4090 - val_loss: 48.7673\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 46.71174\n",
      "Epoch 40/50\n",
      "11212/11212 [==============================] - 7s 581us/step - loss: 36.6848 - val_loss: 49.3635\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 46.71174\n",
      "Epoch 41/50\n",
      "11212/11212 [==============================] - 7s 582us/step - loss: 35.5400 - val_loss: 49.7952\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 46.71174\n",
      "Epoch 42/50\n",
      "11212/11212 [==============================] - 6s 572us/step - loss: 34.7908 - val_loss: 49.6680\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 46.71174\n",
      "Epoch 43/50\n",
      "11212/11212 [==============================] - 6s 579us/step - loss: 33.9314 - val_loss: 50.5270\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 46.71174\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11212/11212 [==============================] - 7s 582us/step - loss: 33.2876 - val_loss: 50.6168\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 46.71174\n",
      "Epoch 45/50\n",
      "11212/11212 [==============================] - 7s 587us/step - loss: 32.5831 - val_loss: 51.2388\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 46.71174\n",
      "Epoch 46/50\n",
      "11212/11212 [==============================] - 7s 592us/step - loss: 32.2355 - val_loss: 51.8013\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 46.71174\n",
      "Epoch 47/50\n",
      "11212/11212 [==============================] - 7s 589us/step - loss: 31.7718 - val_loss: 52.2610\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 46.71174\n",
      "Epoch 48/50\n",
      "11212/11212 [==============================] - 7s 586us/step - loss: 31.1786 - val_loss: 53.1098\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 46.71174\n",
      "Epoch 49/50\n",
      "11212/11212 [==============================] - 7s 586us/step - loss: 30.4485 - val_loss: 53.2708\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 46.71174\n",
      "Epoch 50/50\n",
      "11212/11212 [==============================] - 7s 591us/step - loss: 30.1735 - val_loss: 53.6945\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 46.71174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ed41c40b8>"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32 ,epochs=50, verbose=1, validation_split=0.2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 3519)              0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 1024)              3604480   \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,889,825\n",
      "Trainable params: 3,887,105\n",
      "Non-trainable params: 2,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weight_model='./model_baseline_test1.h5'\n",
    "model = create_model()\n",
    "model.load_weights(weight_model)\n",
    "# model._make_predict_function()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = y_pred.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, testY):\n",
    "    diff = preds.flatten() - testY\n",
    "    percentDiff = (diff / testY) * 100\n",
    "    absPercentDiff = np.abs(percentDiff)\n",
    "    mean = np.mean(absPercentDiff)\n",
    "    std = np.std(absPercentDiff)\n",
    "    print(mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.91694190528806 26.288253607643497\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07608696] [0.11794312]\n",
      "[266.] [412.32916]\n",
      "[0.24056064] [0.11311649]\n",
      "[841.] [395.45523]\n",
      "[0.82122426] [0.1304482]\n",
      "[2871.] [456.04694]\n",
      "[0.08352403] [0.12217142]\n",
      "[292.] [427.11127]\n",
      "[0.75257437] [0.12687275]\n",
      "[2631.] [443.54712]\n",
      "[0.12156751] [0.142565]\n",
      "[425.] [498.40723]\n",
      "[0.14273455] [0.12378053]\n",
      "[499.] [432.73676]\n",
      "[0.20080092] [0.11713792]\n",
      "[702.] [409.5142]\n",
      "[0.10983982] [0.12041637]\n",
      "[384.] [420.97562]\n",
      "[0.83066362] [0.142016]\n",
      "[2904.] [496.4879]\n",
      "[0.75028604] [0.11423999]\n",
      "[2623.] [399.383]\n",
      "[0.29776888] [0.13957931]\n",
      "[1041.] [487.96927]\n",
      "[0.09868421] [0.11690778]\n",
      "[345.] [408.7096]\n",
      "[0.2271167] [0.13127458]\n",
      "[794.] [458.93594]\n",
      "[0.57494279] [0.1223107]\n",
      "[2010.] [427.5982]\n",
      "[0.18363844] [0.1194088]\n",
      "[642.] [417.45316]\n",
      "[0.21424485] [0.12360439]\n",
      "[749.] [432.12094]\n",
      "[0.26058352] [0.11448178]\n",
      "[911.] [400.2283]\n",
      "[0.61441648] [0.12002429]\n",
      "[2148.] [419.60492]\n",
      "[0.12757437] [0.18252626]\n",
      "[446.] [638.1118]\n",
      "[0.73398169] [0.11766489]\n",
      "[2566.] [411.35645]\n",
      "[0.31207094] [0.12444988]\n",
      "[1091.] [435.07678]\n",
      "[0.46996568] [0.11398606]\n",
      "[1643.] [398.49527]\n",
      "[0.5569222] [0.1162174]\n",
      "[1947.] [406.29605]\n",
      "[0.41618993] [0.11995466]\n",
      "[1455.] [419.36148]\n",
      "[0.25943936] [0.11362187]\n",
      "[907.] [397.22205]\n",
      "[0.90818078] [0.11506152]\n",
      "[3175.] [402.25507]\n",
      "[0.40217391] [0.12021567]\n",
      "[1406.] [420.274]\n",
      "[0.22854691] [0.11554882]\n",
      "[799.] [403.95868]\n",
      "[0.84267735] [0.18853459]\n",
      "[2946.] [659.11694]\n",
      "[0.23598398] [0.11953513]\n",
      "[825.] [417.89484]\n",
      "[0.81836384] [0.11884706]\n",
      "[2861.] [415.48935]\n",
      "[0.81464531] [0.11811157]\n",
      "[2848.] [412.91803]\n",
      "[0.44450801] [0.11742704]\n",
      "[1554.] [410.52493]\n",
      "[0.08094966] [0.13673985]\n",
      "[283.] [478.0425]\n",
      "[0.84153318] [0.12263661]\n",
      "[2942.] [428.73758]\n",
      "[0.32751716] [0.11805458]\n",
      "[1145.] [412.7188]\n",
      "[0.80377574] [0.121504]\n",
      "[2810.] [424.77798]\n",
      "[0.6006865] [0.1186863]\n",
      "[2100.] [414.92728]\n",
      "[0.54576659] [0.11477396]\n",
      "[1908.] [401.24976]\n",
      "[0.16990847] [0.12443863]\n",
      "[594.] [435.03745]\n",
      "[0.30749428] [0.11846788]\n",
      "[1075.] [414.16373]\n",
      "[0.10011442] [0.11625664]\n",
      "[350.] [406.4332]\n",
      "[0.25686499] [0.11298347]\n",
      "[898.] [394.9902]\n",
      "[0.12070938] [0.12124652]\n",
      "[422.] [423.8778]\n",
      "[0.12299771] [0.12971236]\n",
      "[430.] [453.4744]\n",
      "[0.15503432] [0.12927747]\n",
      "[542.] [451.95404]\n",
      "[0.21052632] [0.11727601]\n",
      "[736.] [409.99695]\n",
      "[0.1833524] [0.11659309]\n",
      "[641.] [407.60947]\n",
      "[0.12843249] [0.11295459]\n",
      "[449.] [394.88922]\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(y_test[i],y_pred[i])\n",
    "    print(y_test[i]*MAXVIEW,y_pred[i]*MAXVIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
