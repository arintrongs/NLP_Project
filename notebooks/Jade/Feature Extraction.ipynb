{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change The Parameters HERE!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = '../../data/Thaipbs-tokenize_include_stop.csv'\n",
    "output_csv_path = '../../data/thaipbs_data_with_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "      <th>DOW</th>\n",
       "      <th>time</th>\n",
       "      <th>view</th>\n",
       "      <th>category</th>\n",
       "      <th>tag</th>\n",
       "      <th>hour</th>\n",
       "      <th>numTag</th>\n",
       "      <th>token</th>\n",
       "      <th>numToken</th>\n",
       "      <th>numChar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ฝุ่น PM2.5 : ทส.เตรียมตั้งศูนย์แก้ปัญหาหมอกควั...</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:37</td>\n",
       "      <td>177</td>\n",
       "      <td>สิ่งแวดล้อม</td>\n",
       "      <td>ฝุ่นเชียงใหม่,ฝุ่นPM2.5,ฝุ่นคลุมเมือง,เชียงใหม...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>ฝุ่น,PM,ทส.,เตรียม,ตั้ง,ศูนย์,แก้,ปัญหา,หมอก,ค...</td>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>เลือกตั้ง 2562 : \"เพื่อไทย\" เตรียมยื่น กกต.จัด...</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:20</td>\n",
       "      <td>702</td>\n",
       "      <td>การเมือง</td>\n",
       "      <td>เลือกตั้ง62,เพื่อ่ไทย,กกต.,นับคะแนน,ไทยพีบีเอส...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>เลือกตั้ง,2562,เพื่อ,ไทย,เตรียม,ยื่น,กกต.,จัด,...</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>เตรียมเอาผิด รพ.เอกชน 58 แห่งไม่ส่งข้อมูลราคายา</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:18</td>\n",
       "      <td>583</td>\n",
       "      <td>สาธารณสุข</td>\n",
       "      <td>ยา,เวชภัณฑ์ฯ,โรงพยาบาล,ค่าบริการทางการแพทย์,กร...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>เตรียม,เอา,ผิด,รพ.เอกชน,แห่ง,ไม่,ส่ง,ข้อมูล,รา...</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ตั้งข้อสังเกตปลดล็อกกัญชา เอื้อนายทุนหรือไม่?</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:16</td>\n",
       "      <td>928</td>\n",
       "      <td>อาชญากรรม</td>\n",
       "      <td>กัญชา,มูลนิธิข้าวขวัญ,ThaiPBSnews</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>ตั้ง,ข้อ,สังเกต,ปลด,ล็อก,กัญชา,เอื้อ,นาย,ทุน,ห...</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>คำพิพากษาเต็ม \"เปรมชัย\" หลุดคดีครอบครองซากเสือดำ</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:29</td>\n",
       "      <td>5163</td>\n",
       "      <td>สิ่งแวดล้อม</td>\n",
       "      <td>เสือดำ,เปรมชัย,เขตรักษาพันธุ์สัตว์ป่าทุ่งใหญ่น...</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>คำ,พิพากษา,เต็ม,เปรมชัย,หลุดคดี,ครอบครอง,ซาก,เ...</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                            headline        date  DOW   time  \\\n",
       "0  ฝุ่น PM2.5 : ทส.เตรียมตั้งศูนย์แก้ปัญหาหมอกควั...  05/04/2562  FRI  19:37   \n",
       "1  เลือกตั้ง 2562 : \"เพื่อไทย\" เตรียมยื่น กกต.จัด...  05/04/2562  FRI  19:20   \n",
       "2    เตรียมเอาผิด รพ.เอกชน 58 แห่งไม่ส่งข้อมูลราคายา  05/04/2562  FRI  19:18   \n",
       "3      ตั้งข้อสังเกตปลดล็อกกัญชา เอื้อนายทุนหรือไม่?  05/04/2562  FRI  19:16   \n",
       "4   คำพิพากษาเต็ม \"เปรมชัย\" หลุดคดีครอบครองซากเสือดำ  05/04/2562  FRI  19:29   \n",
       "\n",
       "   view     category                                                tag  hour  \\\n",
       "0   177  สิ่งแวดล้อม  ฝุ่นเชียงใหม่,ฝุ่นPM2.5,ฝุ่นคลุมเมือง,เชียงใหม...    19   \n",
       "1   702     การเมือง  เลือกตั้ง62,เพื่อ่ไทย,กกต.,นับคะแนน,ไทยพีบีเอส...    19   \n",
       "2   583    สาธารณสุข  ยา,เวชภัณฑ์ฯ,โรงพยาบาล,ค่าบริการทางการแพทย์,กร...    19   \n",
       "3   928    อาชญากรรม                  กัญชา,มูลนิธิข้าวขวัญ,ThaiPBSnews    19   \n",
       "4  5163  สิ่งแวดล้อม  เสือดำ,เปรมชัย,เขตรักษาพันธุ์สัตว์ป่าทุ่งใหญ่น...    19   \n",
       "\n",
       "   numTag                                              token  numToken  \\\n",
       "0      10  ฝุ่น,PM,ทส.,เตรียม,ตั้ง,ศูนย์,แก้,ปัญหา,หมอก,ค...        12   \n",
       "1       6  เลือกตั้ง,2562,เพื่อ,ไทย,เตรียม,ยื่น,กกต.,จัด,...        12   \n",
       "2       9  เตรียม,เอา,ผิด,รพ.เอกชน,แห่ง,ไม่,ส่ง,ข้อมูล,รา...        10   \n",
       "3       3  ตั้ง,ข้อ,สังเกต,ปลด,ล็อก,กัญชา,เอื้อ,นาย,ทุน,ห...        11   \n",
       "4       5  คำ,พิพากษา,เต็ม,เปรมชัย,หลุดคดี,ครอบครอง,ซาก,เ...         8   \n",
       "\n",
       "   numChar  \n",
       "0       47  \n",
       "1       58  \n",
       "2       42  \n",
       "3       43  \n",
       "4       44  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from stop_words import get_stop_words\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = get_stop_words('en')\n",
    "num = [str(i) for i in range(1000)]\n",
    "stop_words = list(thai_stopwords())\n",
    "stop_words+=[' ','']\n",
    "stop_words+=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getToken(text):\n",
    "    res = word_tokenize(text,engine='deepcut')\n",
    "    res = [str(i).encode('utf-8') for i in res if i not in en_stop and i not in num and i.strip() not in stop_words]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['token'] = data['headline'].apply(getToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['numToken'] = data['token'].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['numChar'] = data['token'].apply(lambda x: len(''.join([i.decode('utf-8') for i in x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['token'] = data['token'].apply(lambda x: [i.decode('utf-8') for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['token'] = data['token'].apply(lambda x : [i.replace('“,', '') for i in x])\n",
    "data['token'] = data['token'].apply(lambda x : [i.replace('”', ',') for i in x])\n",
    "data['token'] = data['token'].apply(lambda x : [i.replace(',“', ',') for i in x])\n",
    "data['token'] = data['token'].apply(lambda x : [i.replace(',,', ',') for i in x])\n",
    "data['token'] = data['token'].apply(lambda x : [i.replace('“', ',') for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Junk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date & Timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data['date'].apply(lambda x : x.split('/')[0]\n",
    "                                            +'/'+x.split('/')[1]+'/'+str(int(x.split('/')[2])-543))\n",
    "data['date'] = pd.to_datetime(data['date'],format=\"%d/%m/%Y\")\n",
    "data['timeDelta'] = pd.to_datetime('20/04/2019',format=\"%d/%m/%Y\")- data['date']\n",
    "data['timeDelta'] = data['timeDelta'].apply(lambda x: int(str(x).split(' ')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_noun'] = 0\n",
    "data['num_pronoun'] = 0\n",
    "data['num_verb'] = 0\n",
    "data['num_preverb'] = 0\n",
    "data['num_determiner'] = 0\n",
    "data['num_adverb'] = 0\n",
    "data['num_classifier'] = 0\n",
    "data['num_conjunction'] = 0\n",
    "data['num_preposition'] = 0\n",
    "data['num_interjunction'] = 0\n",
    "data['num_prefix'] = 0\n",
    "data['num_ending'] = 0\n",
    "data['num_negator'] = 0\n",
    "data['num_punctuation'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"''\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-6e5fd3726765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"''\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'artagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'NEG'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NEG'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/pythainlp/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(words, engine, corpus)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# default, use \"unigram\" (\"old\") engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0munigram\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtag_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0m_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_corpus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"orchid_ud\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/pythainlp/tag/__init__.py\u001b[0m in \u001b[0;36m_artagger_tag\u001b[0;34m(words, corpus)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0martagger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mwords_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/artagger/__init__.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructSCRDRtreeFromRDRfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdr_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagRawSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/artagger/__init__.py\u001b[0m in \u001b[0;36mtagRawSentence\u001b[0;34m(self, dictionary, rawLine)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtagRawSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawLine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializeSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawLine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0msen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mwordTags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.0/lib/python3.6/site-packages/artagger/InitialTagger/InitialTagger.py\u001b[0m in \u001b[0;36minitializeSentence\u001b[0;34m(FREQDICT, sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"“\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"”\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtaggedSen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"''/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFREQDICT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"''\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"''\""
     ]
    }
   ],
   "source": [
    "for index, row in data.iterrows():\n",
    "    pos = pos_tag([i.replace(\"\\''\",'') for i in row['token']],engine='artagger')\n",
    "    for token,p in pos:\n",
    "        if p[0] =='NEG':\n",
    "            data.at[index,'NEG'] += 1\n",
    "        elif p[0] =='N':\n",
    "            data.at[index,'num_noun'] += 1\n",
    "        elif p[0] =='PUNC':\n",
    "            data.at[index,'num_punctuation'] += 1\n",
    "        elif p[0] =='P':\n",
    "            data.at[index,'num_pronoun'] += 1\n",
    "        elif p[0] =='V':\n",
    "            data.at[index,'num_verb'] += 1\n",
    "        elif p[0] =='X':\n",
    "            data.at[index,'num_preverb'] += 1\n",
    "        elif p[0] =='D':\n",
    "            data.at[index,'num_determiner'] += 1\n",
    "        elif p[0] =='A':\n",
    "            data.at[index,'num_adverb'] += 1\n",
    "        elif p[0] =='C':\n",
    "            data.at[index,'num_classifier'] += 1\n",
    "        elif p[0] =='J':\n",
    "            data.at[index,'num_conjunction'] += 1\n",
    "        elif p[0] =='R':\n",
    "            data.at[index,'num_preposition'] += 1\n",
    "        elif p[0] =='I':\n",
    "            data.at[index,'num_interjunction'] += 1\n",
    "        elif p[0] =='F':\n",
    "            data.at[index,'num_prefix'] += 1\n",
    "        elif p[0] =='E':\n",
    "            data.at[index,'num_ending'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nnoun_per_ntoken'] = data['num_noun']/data['numToken']\n",
    "data['nverb_per_ntoken'] = data['num_verb']/data['numToken']\n",
    "data['nadverb_per_ntoken'] = data['num_adverb']/data['numToken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag / Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_category = set()\n",
    "for i in data['category']:\n",
    "    for j in str(i).split(','):\n",
    "        unique_category.add(j.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(unique_category):\n",
    "    data['is_'+i] = data.apply(lambda row : int(i in str(row['category'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tag_pop_sum'] = 0\n",
    "data['tag_pop_mean'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = dict()\n",
    "for index, row in data.iterrows():\n",
    "    tag = str(row['tag']).split(',')\n",
    "    for t in tag:\n",
    "        if t not in tag_dict.keys():\n",
    "            tag_dict[t] = 1\n",
    "        else:\n",
    "            tag_dict[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    tag = str(row['tag']).split(',')\n",
    "    pop_list = [tag_dict[t] for t in tag if t in tag_dict.keys()]\n",
    "    data.at[index,'tag_pop_sum'] = sum(pop_list)\n",
    "    if len(pop_list) > 0:\n",
    "        data.at[index,'tag_pop_mean'] = sum(pop_list)/len(pop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tag.named_entity import ThaiNameTagger\n",
    "ner = ThaiNameTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_abb = pd.read_csv('../Eye/thai_abb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "abb_set = set(thai_abb['อักษรย่อ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_date'] = 0\n",
    "data['num_email'] = 0\n",
    "data['num_law'] = 0\n",
    "data['num_len'] = 0\n",
    "data['num_location'] = 0\n",
    "data['num_money'] = 0\n",
    "data['num_organization'] = 0\n",
    "data['num_percent'] = 0\n",
    "data['num_person'] = 0\n",
    "data['num_phone'] = 0\n",
    "data['num_time'] = 0\n",
    "data['num_url'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    token = row['token']\n",
    "    for t in token:\n",
    "        if t in abb_set:\n",
    "            t = list(thai_abb[thai_abb['อักษรย่อ']==t]['ชื่อเต็ม'])[0]\n",
    "        elif t.split('.')[0] in abb_set:\n",
    "            t = list(thai_abb[thai_abb['อักษรย่อ']==t.split('.')[0]]['ชื่อเต็ม'])[0]\n",
    "        NER = ner.get_ner(t)\n",
    "        if len(NER) > 1:\n",
    "            word,pos,n = NER[1]\n",
    "        else:\n",
    "            word,pos,n = NER[0]  \n",
    "        if n == 'I-DATE':\n",
    "            data.at[index,'num_date'] += 1\n",
    "        elif n == 'I-EMAIL':\n",
    "            data.at[index,'num_email'] += 1\n",
    "        elif n == 'I-LAW':\n",
    "            data.at[index,'num_law'] += 1\n",
    "        elif n == 'I-LEN':\n",
    "            data.at[index,'num_len'] += 1\n",
    "        elif n == 'I-LOCATION':\n",
    "            data.at[index,'num_location'] += 1\n",
    "        elif n == 'I-MONEY':\n",
    "            data.at[index,'num_money'] += 1\n",
    "        elif n == 'I-ORGANIZATION':\n",
    "            data.at[index,'num_organization'] += 1\n",
    "        elif n == 'I-PERCENT':\n",
    "            data.at[index,'num_percent'] += 1\n",
    "        elif n == 'I-PERSON':\n",
    "            data.at[index,'num_person'] += 1\n",
    "        elif n == 'I-PHONE':\n",
    "            data.at[index,'num_phone'] += 1\n",
    "        elif n == 'I-TIME':\n",
    "            data.at[index,'num_time'] += 1\n",
    "        elif n == 'I-URL':\n",
    "            data.at[index,'num_url'] += 1         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thai Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_name = pd.read_csv('../Eye/ThaiName.txt',sep=\" \",header=None)\n",
    "thai_name.columns = [\"firstName\", \"lastName\"]\n",
    "thai_name.dropna(inplace=True)\n",
    "thai_name_f = pd.read_csv('../Eye/ThaiName-female.txt',sep=\" \",header=None)\n",
    "thai_name_f.columns = [\"firstName\"]\n",
    "thai_name_f.dropna(inplace=True)\n",
    "thai_name_m = pd.read_csv('../Eye/ThaiName-man.txt',sep=\" \",header=None)\n",
    "thai_name_m.columns = [\"firstName\"]\n",
    "thai_name_m.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "thaiNameSet = set(thai_name_f['firstName']).union(set(thai_name_m['firstName']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_person_2'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserve_name = ['สมัคร','ชวน','พายุ','เลื่อน','อำนาจ','สัญญา','นิยม']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    token = row['token']\n",
    "    for t in token : \n",
    "        if t in thaiNameSet and t not in reserve_name:\n",
    "            data.at[index,'num_person_2'] += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df1 = pd.read_csv('../Eye/sentiment_data/positive_new.txt', header=None)\n",
    "pos_df1.columns = [\"pos_word\"]\n",
    "pos_df1.dropna(inplace=True)\n",
    "pos_df2 = pd.read_csv('../Eye/sentiment_data/positive__verbs.txt', header=None)\n",
    "pos_df2.columns = [\"pos_word\"]\n",
    "pos_df2.dropna(inplace=True)\n",
    "pos_df3 = pd.read_csv('../Eye/sentiment_data/positive__adjectives.txt', header=None)\n",
    "pos_df3.columns = [\"pos_word\"]\n",
    "pos_df3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df1 = pd.read_csv('../Eye/sentiment_data/negative_new.txt', header=None)\n",
    "neg_df1.columns = [\"neg_word\"]\n",
    "neg_df1.dropna(inplace=True)\n",
    "neg_df2 = pd.read_csv('../Eye/sentiment_data/negative_verbs.txt', header=None)\n",
    "neg_df2.columns = [\"neg_word\"]\n",
    "neg_df2.dropna(inplace=True)\n",
    "neg_df3 = pd.read_csv('../Eye/sentiment_data/negative_adjectives.txt', header=None)\n",
    "neg_df3.columns = [\"neg_word\"]\n",
    "neg_df3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_set = set(pos_df1['pos_word']).union(set(pos_df2['pos_word'])).union(set(pos_df3['pos_word']))\n",
    "neg_word_set = set(neg_df1['neg_word']).union(set(neg_df2['neg_word'])).union(set(neg_df3['neg_word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_pos'] = 0\n",
    "data['num_neg'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    token = row['token']\n",
    "    for t in token : \n",
    "        if t in pos_word_set:\n",
    "            data.at[index,'num_pos'] += 1  \n",
    "        elif t in neg_word_set:\n",
    "            data.at[index,'num_neg'] += 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tag_pop_sum'] = 0\n",
    "data['tag_pop_mean'] = 0\n",
    "data['tag_top_pop_sum'] = 0\n",
    "data['tag_top_pop_mean'] = 0\n",
    "data['num_tag_pop'] = 0\n",
    "data['tag_top_worst_sum'] = 0\n",
    "data['tag_top_worst_mean'] = 0\n",
    "data['num_tag_worst'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['numTag'] = data.apply(lambda x: len(str(x['tag']).split(',')),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    tag = str(row['tag']).split(',')\n",
    "    pop_list = [tag_dict[t] for t in tag if t in tag_dict.keys()]\n",
    "    data.at[index,'tag_pop_sum'] = sum(pop_list)\n",
    "    if len(pop_list) > 0:\n",
    "        data.at[index,'tag_pop_mean'] = sum(pop_list)/len(pop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_news = pd.DataFrame(data.sort_values('view',ascending=False)[:1000])\n",
    "worst_news = pd.DataFrame(data.sort_values('view',ascending=True)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict_top = dict()\n",
    "for index, row in top_news.iterrows():\n",
    "    tag = str(row['tag']).split(',')\n",
    "    for t in tag:\n",
    "        if t not in tag_dict_top.keys():\n",
    "            tag_dict_top[t] = 1\n",
    "        else:\n",
    "            tag_dict_top[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict_worst = dict()\n",
    "for index, row in worst_news.iterrows():\n",
    "    tag = str(row['tag']).split(',')\n",
    "    for t in tag:\n",
    "        if t not in tag_dict_worst.keys():\n",
    "            tag_dict_worst[t] = 1\n",
    "        else:\n",
    "            tag_dict_worst[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    tag = str(row['tag']).split(',')\n",
    "    pop_list = [tag_dict_top[t] for t in tag if t in tag_dict_top.keys()]\n",
    "    data.at[index,'num_tag_pop'] = len(pop_list)\n",
    "    data.at[index,'tag_top_pop_sum'] = sum(pop_list)\n",
    "    if len(pop_list) > 0:\n",
    "        data.at[index,'tag_top_pop_mean'] = sum(pop_list)/len(pop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    tag = str(row['tag']).split(',')\n",
    "    worst_list = [tag_dict_worst[t] for t in tag if t in tag_dict_worst.keys()]\n",
    "    data.at[index,'num_tag_worst'] = len(worst_list)\n",
    "    data.at[index,'tag_top_worst_sum'] = sum(worst_list)\n",
    "    if len(worst_list) > 0:\n",
    "        data.at[index,'tag_top_worst_mean'] = sum(worst_list)/len(worst_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arintrongs2/.pyenv/versions/3.6.0/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pythainlp.word_vector import sentence_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxSim(embed1, headline_embed):\n",
    "    a = np.asarray([cosine_similarity(embed1,x) for x in headline_embed])\n",
    "    return a.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_headline = np.asarray(pd.DataFrame(data.sort_values('view',ascending=False)[:1000]['headline'])['headline'])\n",
    "top_headline_embed = np.asarray([sentence_vectorizer(x) for x in top_headline])\n",
    "worst_headline = np.asarray(pd.DataFrame(data.sort_values('view',ascending=True)[:1000]['headline'])['headline'])\n",
    "worst_headline_embed = np.asarray([sentence_vectorizer(x) for x in worst_headline])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Becareful this cell runs fucking long**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    sent = row['headline']\n",
    "    sent_embed = sentence_vectorizer(sent)\n",
    "    data.at[index,'sim_top'] = findMaxSim(sent_embed,top_headline_embed)\n",
    "    data.at[index,'sim_worst'] = findMaxSim(sent_embed,worst_headline_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(output_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
