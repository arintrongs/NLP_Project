{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tag.named_entity import ThaiNameTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_standard = pd.read_csv('../../data/the_standard_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_pbs = pd.read_csv('../../data/Thaipbs-tokenize_include_stop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arintrongs2/.pyenv/versions/3.6.0/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.concat([the_standard,thai_pbs[:4*len(thai_pbs)//5]])\n",
    "test_data = pd.concat([thai_pbs[4*len(thai_pbs)//5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = ThaiNameTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD :  506.6089725616039\n",
      "MEAN :  663.6907290958532\n",
      "[0.0017565342492673608, 233.0731024184899, 309.06444830273045, 486.3775886992918, 663.6907290958532, 1170.299701657457, 2183.517646780665]\n"
     ]
    }
   ],
   "source": [
    "boundary = 2000\n",
    "data_bounded = train_data[train_data['view']<boundary]\n",
    "x = data_bounded['view']\n",
    "std = np.std(x)\n",
    "mean = np.mean(x)\n",
    "groups = [mean-1*std-157.08,mean-0.85*std,mean-0.7*std,mean-0.35*std,mean,mean+std,mean+3*std]\n",
    "print('STD : ',std)\n",
    "print('MEAN : ',mean)\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_level = []\n",
    "for i in x:\n",
    "    for j in range(len(groups)-1):\n",
    "        if i >= groups[j] and i <= groups[j+1]:\n",
    "            pop_level+=[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5]), array([4718, 2557, 4373, 3044, 4566, 4275]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(pop_level,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = data_bounded['headline']\n",
    "views = data_bounded['view']\n",
    "dow = data_bounded['DOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = set()\n",
    "pos = set()\n",
    "for hl in [ner_tagger.get_ner(i) for i in headlines]:\n",
    "    for word in hl:\n",
    "        ner.add(word[1])\n",
    "        pos.add(word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = list(ner)\n",
    "pos = list(pos)\n",
    "ner_idx = [i for i in range(1,len(ner)+1)]\n",
    "pos_idx = [i for i in range(1,len(pos)+1)]\n",
    "dow_idx = [i for i in range(1,len(list(set(dow)))+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_to_idx = dict(zip(ner,ner_idx))\n",
    "idx_to_ner = {v: k for k, v in ner_to_idx.items()}\n",
    "\n",
    "pos_to_idx = dict(zip(pos,pos_idx))\n",
    "idx_to_pos = {v: k for k, v in pos_to_idx.items()}\n",
    "\n",
    "dow_to_idx = dict(zip(list(set(dow)),dow_idx))\n",
    "idx_to_dow = {v: k for k, v in dow_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_x(data):\n",
    "    data_ner = [ner_tagger.get_ner(i) for i in data]\n",
    "    x = []\n",
    "    \n",
    "    for idx in range(len(data_ner)):\n",
    "        temp = []\n",
    "        for word in data_ner[idx]:\n",
    "            temp+=[ner_to_idx[word[1]]]\n",
    "            temp+=[pos_to_idx[word[2]]]\n",
    "            temp+=[dow_to_idx[dow.iloc[idx]]]\n",
    "        x+=[temp]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_y(data):\n",
    "    y = []\n",
    "    for i in data:\n",
    "        c = 0\n",
    "        for j in range(len(groups)-1):\n",
    "            if i >= groups[j] and i <= groups[j+1]:\n",
    "                y+=[j]\n",
    "                c=1\n",
    "                break\n",
    "        if c==0:\n",
    "            y+=[0]\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape :  (23536, 130)\n",
      "y_train shape :  (23536, 6)\n",
      "x_test shape :  (5595, 130)\n",
      "y_test shape :  (5595, 6)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x_train = prepare_x(headlines)\n",
    "y_train = prepare_y(views)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=130)\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "x_test = prepare_x(test_data['headline'])\n",
    "y_test = prepare_y(test_data['view'])\n",
    "\n",
    "x_test = pad_sequences(x_test, maxlen=130)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print('x_train shape : ',x_train.shape)\n",
    "print('y_train shape : ',y_train.shape)\n",
    "\n",
    "print('x_test shape : ',x_test.shape)\n",
    "print('y_test shape : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, GRU, Conv1D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "def get_model():    \n",
    "    input1 = Input(shape=(x_train.shape[1],))\n",
    "#     x = Conv1D(16,3)(input1)\n",
    "#     x = GRU(32)(x)\n",
    "#     x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(input1)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "#     x = Dense(64, activation='relu')(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "    x = Dense(y_train.shape[1],activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=x)\n",
    "    adam  = Adam(lr=0.00001)\n",
    "    model.compile(optimizer=adam,  loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arintrongs2/.pyenv/versions/3.6.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/arintrongs2/.pyenv/versions/3.6.0/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 130)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              134144    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 791,814\n",
      "Trainable params: 791,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arintrongs2/.pyenv/versions/3.6.0/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 21182 samples, validate on 2354 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 3.6001 - acc: 0.1747 - val_loss: 1.8983 - val_acc: 0.1504\n",
      "Epoch 2/50\n",
      " - 4s - loss: 2.6343 - acc: 0.1760 - val_loss: 1.8119 - val_acc: 0.1640\n",
      "Epoch 3/50\n",
      " - 4s - loss: 2.2677 - acc: 0.1822 - val_loss: 1.8055 - val_acc: 0.1627\n",
      "Epoch 4/50\n",
      " - 4s - loss: 2.0966 - acc: 0.1809 - val_loss: 1.7976 - val_acc: 0.1653\n",
      "Epoch 5/50\n",
      " - 4s - loss: 2.0110 - acc: 0.1827 - val_loss: 1.7971 - val_acc: 0.1563\n",
      "Epoch 6/50\n",
      " - 4s - loss: 1.9567 - acc: 0.1811 - val_loss: 1.7935 - val_acc: 0.1754\n",
      "Epoch 7/50\n",
      " - 4s - loss: 1.9158 - acc: 0.1865 - val_loss: 1.7947 - val_acc: 0.1733\n",
      "Epoch 8/50\n",
      " - 4s - loss: 1.8915 - acc: 0.1812 - val_loss: 1.7934 - val_acc: 0.1763\n",
      "Epoch 9/50\n",
      " - 4s - loss: 1.8702 - acc: 0.1876 - val_loss: 1.7931 - val_acc: 0.1759\n",
      "Epoch 10/50\n",
      " - 4s - loss: 1.8506 - acc: 0.1906 - val_loss: 1.7886 - val_acc: 0.1814\n",
      "Epoch 11/50\n",
      " - 4s - loss: 1.8435 - acc: 0.1880 - val_loss: 1.7896 - val_acc: 0.1746\n",
      "Epoch 12/50\n",
      " - 4s - loss: 1.8286 - acc: 0.1911 - val_loss: 1.7880 - val_acc: 0.1827\n",
      "Epoch 13/50\n",
      " - 4s - loss: 1.8242 - acc: 0.1842 - val_loss: 1.7893 - val_acc: 0.1814\n",
      "Epoch 14/50\n",
      " - 4s - loss: 1.8165 - acc: 0.1883 - val_loss: 1.7899 - val_acc: 0.1907\n",
      "Epoch 15/50\n",
      " - 4s - loss: 1.8119 - acc: 0.1902 - val_loss: 1.7884 - val_acc: 0.1810\n",
      "Epoch 16/50\n",
      " - 4s - loss: 1.8075 - acc: 0.1907 - val_loss: 1.7872 - val_acc: 0.1810\n",
      "Epoch 17/50\n",
      " - 4s - loss: 1.7989 - acc: 0.1929 - val_loss: 1.7873 - val_acc: 0.1661\n",
      "Epoch 18/50\n",
      " - 4s - loss: 1.7979 - acc: 0.1946 - val_loss: 1.7864 - val_acc: 0.1674\n",
      "Epoch 19/50\n",
      " - 4s - loss: 1.7940 - acc: 0.1946 - val_loss: 1.7850 - val_acc: 0.1708\n",
      "Epoch 20/50\n",
      " - 4s - loss: 1.7950 - acc: 0.1940 - val_loss: 1.7854 - val_acc: 0.1725\n",
      "Epoch 21/50\n",
      " - 4s - loss: 1.7903 - acc: 0.1999 - val_loss: 1.7858 - val_acc: 0.1619\n",
      "Epoch 22/50\n",
      " - 4s - loss: 1.7886 - acc: 0.1975 - val_loss: 1.7868 - val_acc: 0.1678\n",
      "Epoch 23/50\n",
      " - 4s - loss: 1.7908 - acc: 0.1970 - val_loss: 1.7872 - val_acc: 0.1619\n",
      "Epoch 24/50\n",
      " - 4s - loss: 1.7859 - acc: 0.2016 - val_loss: 1.7874 - val_acc: 0.1665\n",
      "Epoch 25/50\n",
      " - 4s - loss: 1.7843 - acc: 0.2006 - val_loss: 1.7859 - val_acc: 0.1585\n",
      "Epoch 26/50\n",
      " - 4s - loss: 1.7871 - acc: 0.1970 - val_loss: 1.7865 - val_acc: 0.1580\n",
      "Epoch 27/50\n",
      " - 4s - loss: 1.7830 - acc: 0.2037 - val_loss: 1.7858 - val_acc: 0.1551\n",
      "Epoch 28/50\n",
      " - 4s - loss: 1.7835 - acc: 0.1967 - val_loss: 1.7858 - val_acc: 0.1555\n",
      "Epoch 29/50\n",
      " - 4s - loss: 1.7819 - acc: 0.2001 - val_loss: 1.7859 - val_acc: 0.1512\n",
      "Epoch 30/50\n",
      " - 4s - loss: 1.7815 - acc: 0.2000 - val_loss: 1.7856 - val_acc: 0.1500\n",
      "Epoch 31/50\n",
      " - 4s - loss: 1.7778 - acc: 0.2013 - val_loss: 1.7869 - val_acc: 0.1559\n",
      "Epoch 32/50\n",
      " - 4s - loss: 1.7794 - acc: 0.1987 - val_loss: 1.7859 - val_acc: 0.1602\n",
      "Epoch 33/50\n",
      " - 4s - loss: 1.7774 - acc: 0.1993 - val_loss: 1.7856 - val_acc: 0.1483\n",
      "Epoch 34/50\n",
      " - 4s - loss: 1.7777 - acc: 0.1995 - val_loss: 1.7862 - val_acc: 0.1376\n",
      "Epoch 35/50\n",
      " - 4s - loss: 1.7777 - acc: 0.2009 - val_loss: 1.7862 - val_acc: 0.1504\n",
      "Epoch 36/50\n",
      " - 4s - loss: 1.7755 - acc: 0.2020 - val_loss: 1.7859 - val_acc: 0.1415\n",
      "Epoch 37/50\n",
      " - 4s - loss: 1.7757 - acc: 0.2056 - val_loss: 1.7850 - val_acc: 0.1504\n",
      "Epoch 38/50\n",
      " - 4s - loss: 1.7730 - acc: 0.2092 - val_loss: 1.7849 - val_acc: 0.1487\n",
      "Epoch 39/50\n",
      " - 4s - loss: 1.7753 - acc: 0.2038 - val_loss: 1.7853 - val_acc: 0.1461\n",
      "Epoch 40/50\n",
      " - 4s - loss: 1.7739 - acc: 0.2044 - val_loss: 1.7856 - val_acc: 0.1364\n",
      "Epoch 41/50\n",
      " - 4s - loss: 1.7749 - acc: 0.2029 - val_loss: 1.7852 - val_acc: 0.1359\n",
      "Epoch 42/50\n",
      " - 4s - loss: 1.7714 - acc: 0.2097 - val_loss: 1.7859 - val_acc: 0.1419\n",
      "Epoch 43/50\n",
      " - 4s - loss: 1.7720 - acc: 0.2047 - val_loss: 1.7858 - val_acc: 0.1393\n",
      "Epoch 44/50\n",
      " - 4s - loss: 1.7712 - acc: 0.2043 - val_loss: 1.7861 - val_acc: 0.1440\n",
      "Epoch 45/50\n",
      " - 4s - loss: 1.7732 - acc: 0.2042 - val_loss: 1.7861 - val_acc: 0.1262\n",
      "Epoch 46/50\n",
      " - 4s - loss: 1.7727 - acc: 0.2016 - val_loss: 1.7856 - val_acc: 0.1347\n",
      "Epoch 47/50\n",
      " - 4s - loss: 1.7718 - acc: 0.2061 - val_loss: 1.7863 - val_acc: 0.1283\n",
      "Epoch 48/50\n",
      " - 4s - loss: 1.7713 - acc: 0.2021 - val_loss: 1.7861 - val_acc: 0.1381\n",
      "Epoch 49/50\n",
      " - 4s - loss: 1.7720 - acc: 0.2040 - val_loss: 1.7864 - val_acc: 0.1325\n",
      "Epoch 50/50\n",
      " - 4s - loss: 1.7722 - acc: 0.2059 - val_loss: 1.7851 - val_acc: 0.1406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1cd75cd9b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32 ,epochs=50, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred,y_test):\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    count = [0,0,0,0,0]\n",
    "    for i in range(len(y_pred)):\n",
    "        p_level_pred = np.argmax(y_pred[i])\n",
    "        p_level_test = np.argmax(y_test[i])\n",
    "        count[p_level_pred]+=1\n",
    "        if(p_level_pred == p_level_test):\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong +=1\n",
    "    print(count)\n",
    "    return correct/(correct+wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_level = np.argmax(y_pred[0])\n",
    "# print('Pop Level : ',p_level)\n",
    "# print('Pred Range : %f - %f' % (groups[p_level], groups[p_level+1]))\n",
    "# print('Real View : %d' % (202))\n",
    "print(evaluate(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We test how the model works by creating a dataframe from the sample. The df is then used as source for the seaborn plot below\n",
    "df_someXdata = pd.DataFrame([np.argmax(i) for i in y_pred],[np.argmax(i) for i in y_test])\n",
    "df_someXdata.reset_index(level=0, inplace=True)\n",
    "df_someXdata_LR = df_someXdata.rename(index=str, columns={\"index\": \"Actual views\", 0: \"Predicted views\"})\n",
    "# df_someXdata_LR[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(17, 10))\n",
    "sns.regplot(x=df_someXdata_LR[\"Actual views\"], y=df_someXdata_LR[\"Predicted views\"])\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, GRU, Conv1D, Dropout, TimeDistributed, LSTM\n",
    "from keras.optimizers import Adam\n",
    "def get_model_gru():    \n",
    "    input1 = Input(shape=(x_train.shape[1],))\n",
    "    x = LSTM(32)(input1)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(y_train.shape[1],activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=x)\n",
    "    adam  = Adam(lr=0.00001)\n",
    "    model.compile(optimizer=adam,  loss='categorical_crossentropy' ,metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_gru()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:2',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:3']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
