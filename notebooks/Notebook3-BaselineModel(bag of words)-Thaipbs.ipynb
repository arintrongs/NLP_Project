{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import codecs\n",
    "import collections\n",
    "import keras.preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Embedding,TimeDistributed,Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import GRU, Dropout\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import Embedding,TimeDistributed,Flatten\n",
    "from keras.layers import BatchNormalization,Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pbs = pd.read_csv('Thaipbs-tokenize_include_stop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pbs.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "input_pbs.drop('Unnamed: 1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "      <th>DOW</th>\n",
       "      <th>time</th>\n",
       "      <th>view</th>\n",
       "      <th>category</th>\n",
       "      <th>tag</th>\n",
       "      <th>hour</th>\n",
       "      <th>numTag</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ฝุ่น PM2.5 : ทส.เตรียมตั้งศูนย์แก้ปัญหาหมอกควั...</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:37</td>\n",
       "      <td>177</td>\n",
       "      <td>สิ่งแวดล้อม</td>\n",
       "      <td>ฝุ่นเชียงใหม่,ฝุ่นPM2.5,ฝุ่นคลุมเมือง,เชียงใหม...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>ฝุ่น,PM,ทส.,เตรียม,ตั้ง,ศูนย์,แก้,ปัญหา,หมอก,ค...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>เลือกตั้ง 2562 : \"เพื่อไทย\" เตรียมยื่น กกต.จัด...</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:20</td>\n",
       "      <td>702</td>\n",
       "      <td>การเมือง</td>\n",
       "      <td>เลือกตั้ง62,เพื่อ่ไทย,กกต.,นับคะแนน,ไทยพีบีเอส...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>เลือกตั้ง,2562,เพื่อ,ไทย,เตรียม,ยื่น,กกต.,จัด,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>เตรียมเอาผิด รพ.เอกชน 58 แห่งไม่ส่งข้อมูลราคายา</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:18</td>\n",
       "      <td>583</td>\n",
       "      <td>สาธารณสุข</td>\n",
       "      <td>ยา,เวชภัณฑ์ฯ,โรงพยาบาล,ค่าบริการทางการแพทย์,กร...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>เตรียม,เอา,ผิด,รพ.เอกชน,แห่ง,ไม่,ส่ง,ข้อมูล,รา...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ตั้งข้อสังเกตปลดล็อกกัญชา เอื้อนายทุนหรือไม่?</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:16</td>\n",
       "      <td>928</td>\n",
       "      <td>อาชญากรรม</td>\n",
       "      <td>กัญชา,มูลนิธิข้าวขวัญ,ThaiPBSnews</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>ตั้ง,ข้อ,สังเกต,ปลด,ล็อก,กัญชา,เอื้อ,นาย,ทุน,ห...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>คำพิพากษาเต็ม \"เปรมชัย\" หลุดคดีครอบครองซากเสือดำ</td>\n",
       "      <td>05/04/2562</td>\n",
       "      <td>FRI</td>\n",
       "      <td>19:29</td>\n",
       "      <td>5163</td>\n",
       "      <td>สิ่งแวดล้อม</td>\n",
       "      <td>เสือดำ,เปรมชัย,เขตรักษาพันธุ์สัตว์ป่าทุ่งใหญ่น...</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>คำ,พิพากษา,เต็ม,เปรมชัย,หลุดคดี,ครอบครอง,ซาก,เ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1                                           headline  \\\n",
       "0             0  ฝุ่น PM2.5 : ทส.เตรียมตั้งศูนย์แก้ปัญหาหมอกควั...   \n",
       "1             1  เลือกตั้ง 2562 : \"เพื่อไทย\" เตรียมยื่น กกต.จัด...   \n",
       "2             2    เตรียมเอาผิด รพ.เอกชน 58 แห่งไม่ส่งข้อมูลราคายา   \n",
       "3             3      ตั้งข้อสังเกตปลดล็อกกัญชา เอื้อนายทุนหรือไม่?   \n",
       "4             4   คำพิพากษาเต็ม \"เปรมชัย\" หลุดคดีครอบครองซากเสือดำ   \n",
       "\n",
       "         date  DOW   time  view     category  \\\n",
       "0  05/04/2562  FRI  19:37   177  สิ่งแวดล้อม   \n",
       "1  05/04/2562  FRI  19:20   702     การเมือง   \n",
       "2  05/04/2562  FRI  19:18   583    สาธารณสุข   \n",
       "3  05/04/2562  FRI  19:16   928    อาชญากรรม   \n",
       "4  05/04/2562  FRI  19:29  5163  สิ่งแวดล้อม   \n",
       "\n",
       "                                                 tag  hour  numTag  \\\n",
       "0  ฝุ่นเชียงใหม่,ฝุ่นPM2.5,ฝุ่นคลุมเมือง,เชียงใหม...    19      10   \n",
       "1  เลือกตั้ง62,เพื่อ่ไทย,กกต.,นับคะแนน,ไทยพีบีเอส...    19       6   \n",
       "2  ยา,เวชภัณฑ์ฯ,โรงพยาบาล,ค่าบริการทางการแพทย์,กร...    19       9   \n",
       "3                  กัญชา,มูลนิธิข้าวขวัญ,ThaiPBSnews    19       3   \n",
       "4  เสือดำ,เปรมชัย,เขตรักษาพันธุ์สัตว์ป่าทุ่งใหญ่น...    19       5   \n",
       "\n",
       "                                               token  \n",
       "0  ฝุ่น,PM,ทส.,เตรียม,ตั้ง,ศูนย์,แก้,ปัญหา,หมอก,ค...  \n",
       "1  เลือกตั้ง,2562,เพื่อ,ไทย,เตรียม,ยื่น,กกต.,จัด,...  \n",
       "2  เตรียม,เอา,ผิด,รพ.เอกชน,แห่ง,ไม่,ส่ง,ข้อมูล,รา...  \n",
       "3  ตั้ง,ข้อ,สังเกต,ปลด,ล็อก,กัญชา,เอื้อ,นาย,ทุน,ห...  \n",
       "4  คำ,พิพากษา,เต็ม,เปรมชัย,หลุดคดี,ครอบครอง,ซาก,เ...  "
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQRval = input_pbs['view'].describe().loc['75%']-input_pbs['view'].describe().loc['25%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3499.5\n"
     ]
    }
   ],
   "source": [
    "outlierMin = max(input_pbs['view'].describe().loc['25%']-1.5*IQRval,0)\n",
    "outlierMax = input_pbs['view'].describe().loc['75%']+1.5*IQRval\n",
    "print(outlierMin,outlierMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pbs = input_pbs[(input_pbs['view']<=outlierMax) & (input_pbs['view']>=outlierMin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24511.000000\n",
       "mean         8.492146\n",
       "std          2.570493\n",
       "min          1.000000\n",
       "25%          7.000000\n",
       "50%          8.000000\n",
       "75%         10.000000\n",
       "max         20.000000\n",
       "Name: numToken, dtype: float64"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pbs['numToken'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24511.000000\n",
       "mean       755.928603\n",
       "std        754.956352\n",
       "min         20.000000\n",
       "25%        228.000000\n",
       "50%        454.000000\n",
       "75%       1006.000000\n",
       "max       3496.000000\n",
       "Name: view, dtype: float64"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pbs['view'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3496"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAXVIEW = input_pbs['view'].max()\n",
    "MAXVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = []\n",
    "for sent in input_pbs['token']:\n",
    "    inputText.append(sent.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLabel = []\n",
    "for view in input_pbs['view']:\n",
    "    inputLabel.append(view/MAXVIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24511 24511\n"
     ]
    }
   ],
   "source": [
    "print(len(inputText),len(inputLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sentence in inputText:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "        \n",
    "word_count = list()\n",
    "word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "word_count = word_count[:len(word_count)//4]\n",
    "word_count.append((\"UNK\",0))\n",
    "\n",
    "train_word = set()\n",
    "for i in word_count:\n",
    "    train_word.add(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4872"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token = set()\n",
    "for word in train_word:\n",
    "    all_token.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token = list(all_token)\n",
    "all_token.insert(0,'for padding')\n",
    "all_token.insert(1,'UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4874"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_map = dict(zip(all_token, range(len(all_token))))\n",
    "token_map_reverse = dict(zip(range(len(all_token)),all_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.asarray(input_pbs['token'].str.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word(sent):\n",
    "    global all_token, token_map\n",
    "    result = np.zeros(len(all_token))\n",
    "    np_token = np.array(sent)\n",
    "    str_token, str_token_count = np.unique(np_token, return_counts=True)\n",
    "    for char, count in zip(str_token, str_token_count):\n",
    "        if char not in token_map.keys():\n",
    "            char = 'UNK'\n",
    "        result[token_map[char]] = count\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.vectorize(count_word, otypes=[object])(input_data)\n",
    "x_f1 = np.array([[e for e in sl] for sl in temp.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=keras.preprocessing.sequence.pad_sequences(x_f1, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24511, 6498)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test =  train_test_split(x_train, test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (19608, 6498)\n",
      "test size (4903, 6498)\n"
     ]
    }
   ],
   "source": [
    "print(\"train size\",x_train.shape)\n",
    "print(\"test size\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = inputLabel\n",
    "y_train = np.asarray(y_train).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test =  train_test_split(y_train, test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 19608\n",
      "test size 4903\n"
     ]
    }
   ],
   "source": [
    "print(\"train size\",len(y_train))\n",
    "print(\"test size\",len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():    \n",
    "    input1 = Input(shape=(x_train.shape[1],))\n",
    "    x = Dense(1024, activation='relu')(input1)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)\n",
    "    x = Dense(1,activation='linear')(x)\n",
    "    model = Model(inputs=input1, outputs=x)\n",
    "    adam  = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam,  loss='mean_absolute_percentage_error')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 6498)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1024)              6654976   \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 6,940,321\n",
      "Trainable params: 6,937,601\n",
      "Non-trainable params: 2,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path='./model_baseline_test1.h5'\n",
    "callbacks_list = [\n",
    "        ModelCheckpoint(\n",
    "            weight_path,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15686 samples, validate on 3922 samples\n",
      "Epoch 1/50\n",
      "15686/15686 [==============================] - 13s 857us/step - loss: 1077.7175 - val_loss: 765.1876\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 765.18761, saving model to ./model_baseline_test1.h5\n",
      "Epoch 2/50\n",
      "15686/15686 [==============================] - 10s 615us/step - loss: 808.4291 - val_loss: 474.7513\n",
      "\n",
      "Epoch 00002: val_loss improved from 765.18761 to 474.75132, saving model to ./model_baseline_test1.h5\n",
      "Epoch 3/50\n",
      "15686/15686 [==============================] - 10s 615us/step - loss: 619.9112 - val_loss: 279.1550\n",
      "\n",
      "Epoch 00003: val_loss improved from 474.75132 to 279.15504, saving model to ./model_baseline_test1.h5\n",
      "Epoch 4/50\n",
      "15686/15686 [==============================] - 10s 616us/step - loss: 468.1542 - val_loss: 184.2698\n",
      "\n",
      "Epoch 00004: val_loss improved from 279.15504 to 184.26977, saving model to ./model_baseline_test1.h5\n",
      "Epoch 5/50\n",
      "15686/15686 [==============================] - 10s 613us/step - loss: 352.3628 - val_loss: 124.4977\n",
      "\n",
      "Epoch 00005: val_loss improved from 184.26977 to 124.49765, saving model to ./model_baseline_test1.h5\n",
      "Epoch 6/50\n",
      "15686/15686 [==============================] - 9s 604us/step - loss: 284.9111 - val_loss: 99.2772\n",
      "\n",
      "Epoch 00006: val_loss improved from 124.49765 to 99.27720, saving model to ./model_baseline_test1.h5\n",
      "Epoch 7/50\n",
      "15686/15686 [==============================] - 10s 606us/step - loss: 228.7953 - val_loss: 87.8322\n",
      "\n",
      "Epoch 00007: val_loss improved from 99.27720 to 87.83220, saving model to ./model_baseline_test1.h5\n",
      "Epoch 8/50\n",
      "15686/15686 [==============================] - 9s 605us/step - loss: 194.7794 - val_loss: 79.2116\n",
      "\n",
      "Epoch 00008: val_loss improved from 87.83220 to 79.21160, saving model to ./model_baseline_test1.h5\n",
      "Epoch 9/50\n",
      "15686/15686 [==============================] - 9s 602us/step - loss: 169.4407 - val_loss: 74.8820\n",
      "\n",
      "Epoch 00009: val_loss improved from 79.21160 to 74.88198, saving model to ./model_baseline_test1.h5\n",
      "Epoch 10/50\n",
      "15686/15686 [==============================] - 9s 605us/step - loss: 155.1904 - val_loss: 70.4260\n",
      "\n",
      "Epoch 00010: val_loss improved from 74.88198 to 70.42602, saving model to ./model_baseline_test1.h5\n",
      "Epoch 11/50\n",
      "15686/15686 [==============================] - 9s 605us/step - loss: 142.8822 - val_loss: 67.7898\n",
      "\n",
      "Epoch 00011: val_loss improved from 70.42602 to 67.78985, saving model to ./model_baseline_test1.h5\n",
      "Epoch 12/50\n",
      "15686/15686 [==============================] - 10s 610us/step - loss: 130.8932 - val_loss: 66.8671\n",
      "\n",
      "Epoch 00012: val_loss improved from 67.78985 to 66.86709, saving model to ./model_baseline_test1.h5\n",
      "Epoch 13/50\n",
      "15686/15686 [==============================] - 10s 607us/step - loss: 122.7929 - val_loss: 63.4434\n",
      "\n",
      "Epoch 00013: val_loss improved from 66.86709 to 63.44339, saving model to ./model_baseline_test1.h5\n",
      "Epoch 14/50\n",
      "15686/15686 [==============================] - 9s 602us/step - loss: 117.6760 - val_loss: 63.6924\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 63.44339\n",
      "Epoch 15/50\n",
      "15686/15686 [==============================] - 9s 589us/step - loss: 112.7823 - val_loss: 63.5307\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 63.44339\n",
      "Epoch 16/50\n",
      "15686/15686 [==============================] - 9s 601us/step - loss: 106.2547 - val_loss: 62.1813\n",
      "\n",
      "Epoch 00016: val_loss improved from 63.44339 to 62.18133, saving model to ./model_baseline_test1.h5\n",
      "Epoch 17/50\n",
      "15686/15686 [==============================] - 10s 607us/step - loss: 99.9769 - val_loss: 64.6398\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 62.18133\n",
      "Epoch 18/50\n",
      "15686/15686 [==============================] - 10s 640us/step - loss: 96.6772 - val_loss: 63.1455\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 62.18133\n",
      "Epoch 19/50\n",
      "15686/15686 [==============================] - 10s 608us/step - loss: 94.8979 - val_loss: 61.2957\n",
      "\n",
      "Epoch 00019: val_loss improved from 62.18133 to 61.29566, saving model to ./model_baseline_test1.h5\n",
      "Epoch 20/50\n",
      "15686/15686 [==============================] - 9s 601us/step - loss: 89.1849 - val_loss: 61.0536\n",
      "\n",
      "Epoch 00020: val_loss improved from 61.29566 to 61.05363, saving model to ./model_baseline_test1.h5\n",
      "Epoch 21/50\n",
      "15686/15686 [==============================] - 9s 603us/step - loss: 83.5200 - val_loss: 61.3799\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 61.05363\n",
      "Epoch 22/50\n",
      "15686/15686 [==============================] - 9s 603us/step - loss: 81.4114 - val_loss: 61.8941\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 61.05363\n",
      "Epoch 23/50\n",
      "15686/15686 [==============================] - 9s 603us/step - loss: 77.2359 - val_loss: 61.4475\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 61.05363\n",
      "Epoch 24/50\n",
      "15686/15686 [==============================] - 10s 608us/step - loss: 74.0656 - val_loss: 60.8711\n",
      "\n",
      "Epoch 00024: val_loss improved from 61.05363 to 60.87110, saving model to ./model_baseline_test1.h5\n",
      "Epoch 25/50\n",
      "15686/15686 [==============================] - 9s 605us/step - loss: 71.1165 - val_loss: 61.1998\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 60.87110\n",
      "Epoch 26/50\n",
      "15686/15686 [==============================] - 9s 601us/step - loss: 68.4907 - val_loss: 61.2516\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 60.87110\n",
      "Epoch 27/50\n",
      "15686/15686 [==============================] - 10s 606us/step - loss: 66.1107 - val_loss: 60.9770\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 60.87110\n",
      "Epoch 28/50\n",
      "15686/15686 [==============================] - 10s 634us/step - loss: 63.8757 - val_loss: 61.5158\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 60.87110\n",
      "Epoch 29/50\n",
      "15686/15686 [==============================] - 10s 626us/step - loss: 61.4591 - val_loss: 61.8231\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 60.87110\n",
      "Epoch 30/50\n",
      "15686/15686 [==============================] - 10s 609us/step - loss: 59.6365 - val_loss: 62.6858\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 60.87110\n",
      "Epoch 31/50\n",
      "15686/15686 [==============================] - 10s 611us/step - loss: 58.3600 - val_loss: 62.2430\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 60.87110\n",
      "Epoch 32/50\n",
      "15686/15686 [==============================] - 10s 611us/step - loss: 56.2632 - val_loss: 62.7897\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 60.87110\n",
      "Epoch 33/50\n",
      "15686/15686 [==============================] - 10s 611us/step - loss: 54.6582 - val_loss: 62.2655\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 60.87110\n",
      "Epoch 34/50\n",
      "15686/15686 [==============================] - 10s 607us/step - loss: 53.3661 - val_loss: 62.6507\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 60.87110\n",
      "Epoch 35/50\n",
      "15686/15686 [==============================] - 9s 602us/step - loss: 51.9580 - val_loss: 62.8220\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 60.87110\n",
      "Epoch 36/50\n",
      "15686/15686 [==============================] - 9s 602us/step - loss: 50.6314 - val_loss: 63.4102\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 60.87110\n",
      "Epoch 37/50\n",
      "15686/15686 [==============================] - 10s 613us/step - loss: 49.9962 - val_loss: 63.0720\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 60.87110\n",
      "Epoch 38/50\n",
      "15686/15686 [==============================] - 10s 613us/step - loss: 48.7274 - val_loss: 63.6536\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 60.87110\n",
      "Epoch 39/50\n",
      "15686/15686 [==============================] - 9s 603us/step - loss: 47.6817 - val_loss: 63.3276\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 60.87110\n",
      "Epoch 40/50\n",
      "15686/15686 [==============================] - 9s 602us/step - loss: 46.4708 - val_loss: 63.5664\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 60.87110\n",
      "Epoch 41/50\n",
      "15686/15686 [==============================] - 9s 603us/step - loss: 45.9633 - val_loss: 64.4287\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 60.87110\n",
      "Epoch 42/50\n",
      "15686/15686 [==============================] - 10s 613us/step - loss: 44.9088 - val_loss: 64.7641\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 60.87110\n",
      "Epoch 43/50\n",
      "15686/15686 [==============================] - 10s 618us/step - loss: 43.9243 - val_loss: 65.8650\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 60.87110\n",
      "Epoch 44/50\n",
      "15686/15686 [==============================] - 9s 603us/step - loss: 43.2808 - val_loss: 67.2581\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 60.87110\n",
      "Epoch 45/50\n",
      "15686/15686 [==============================] - 9s 603us/step - loss: 42.6763 - val_loss: 67.5006\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 60.87110\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15686/15686 [==============================] - 10s 609us/step - loss: 41.8682 - val_loss: 68.0848\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 60.87110\n",
      "Epoch 47/50\n",
      "15686/15686 [==============================] - 9s 599us/step - loss: 41.4253 - val_loss: 68.4809\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 60.87110\n",
      "Epoch 48/50\n",
      "15686/15686 [==============================] - 9s 598us/step - loss: 40.8983 - val_loss: 68.7139\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 60.87110\n",
      "Epoch 49/50\n",
      "15686/15686 [==============================] - 9s 603us/step - loss: 40.4464 - val_loss: 69.1943\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 60.87110\n",
      "Epoch 50/50\n",
      "15686/15686 [==============================] - 9s 602us/step - loss: 39.4271 - val_loss: 69.8115\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 60.87110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb71cf74dd8>"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32 ,epochs=50, verbose=1, validation_split=0.2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15686 samples, validate on 3922 samples\n",
      "Epoch 1/5\n",
      "15686/15686 [==============================] - 9s 595us/step - loss: 36.5602 - val_loss: 72.7645\n",
      "Epoch 2/5\n",
      "  320/15686 [..............................] - ETA: 8s - loss: 33.7238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/burin/.env/src/keras/keras/callbacks.py:707: RuntimeWarning: Can save best model only with train_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15686/15686 [==============================] - 9s 584us/step - loss: 36.2576 - val_loss: 73.2490\n",
      "Epoch 3/5\n",
      "15686/15686 [==============================] - 9s 588us/step - loss: 36.2094 - val_loss: 73.4807\n",
      "Epoch 4/5\n",
      "15686/15686 [==============================] - 9s 585us/step - loss: 35.4603 - val_loss: 74.0365\n",
      "Epoch 5/5\n",
      "15686/15686 [==============================] - 9s 555us/step - loss: 35.9419 - val_loss: 75.4226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6cdc47550>"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32 ,epochs=5, verbose=1, validation_split=0.2,callbacks=callbacks_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_model='./model_baseline_test2.h5'\n",
    "model = create_model()\n",
    "model.load_weights(weight_model)\n",
    "model._make_predict_function()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = y_pred.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, testY):\n",
    "    diff = preds.flatten() - testY\n",
    "    percentDiff = (diff / testY) * 100\n",
    "    absPercentDiff = np.abs(percentDiff)\n",
    "    mean = np.mean(absPercentDiff)\n",
    "    std = np.std(absPercentDiff)\n",
    "    print(mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.73933719442356 147.63111233841977\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.36952848213208 128.7975869594508\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_pred2,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104.] [0.04379225]\n",
      "[104.] [153.09772]\n",
      "[97.] [0.09940206]\n",
      "[97.] [347.50958]\n",
      "[94.] [0.05182158]\n",
      "[94.] [181.16824]\n",
      "[165.] [0.05189327]\n",
      "[165.] [181.41887]\n",
      "[724.] [0.11034767]\n",
      "[724.] [385.77545]\n",
      "[254.] [0.08795989]\n",
      "[254.] [307.50778]\n",
      "[1483.] [0.04845521]\n",
      "[1483.] [169.39943]\n",
      "[1490.] [0.1522605]\n",
      "[1490.] [532.3027]\n",
      "[360.] [0.11586411]\n",
      "[360.] [405.06094]\n",
      "[2805.] [0.11565819]\n",
      "[2805.] [404.34103]\n",
      "[3094.] [0.2553892]\n",
      "[3094.] [892.8407]\n",
      "[184.] [0.08681615]\n",
      "[184.] [303.50928]\n",
      "[231.] [0.16967946]\n",
      "[231.] [593.1994]\n",
      "[364.] [0.1624019]\n",
      "[364.] [567.757]\n",
      "[393.] [0.11865829]\n",
      "[393.] [414.82938]\n",
      "[479.] [0.10168976]\n",
      "[479.] [355.50742]\n",
      "[210.] [0.05912689]\n",
      "[210.] [206.70763]\n",
      "[423.] [0.0515598]\n",
      "[423.] [180.25307]\n",
      "[1278.] [0.07914282]\n",
      "[1278.] [276.68332]\n",
      "[2179.] [0.21793026]\n",
      "[2179.] [761.88416]\n",
      "[130.] [0.05533054]\n",
      "[130.] [193.43558]\n",
      "[73.] [0.06416036]\n",
      "[73.] [224.30463]\n",
      "[351.] [0.1459571]\n",
      "[351.] [510.26602]\n",
      "[127.] [0.05311427]\n",
      "[127.] [185.68747]\n",
      "[1788.] [0.1731593]\n",
      "[1788.] [605.3649]\n",
      "[195.] [0.06459923]\n",
      "[195.] [225.83891]\n",
      "[164.] [0.08727887]\n",
      "[164.] [305.12692]\n",
      "[211.] [0.15559727]\n",
      "[211.] [543.9681]\n",
      "[979.] [0.0619997]\n",
      "[979.] [216.75096]\n",
      "[470.] [0.11098154]\n",
      "[470.] [387.99146]\n",
      "[215.] [0.04756558]\n",
      "[215.] [166.28926]\n",
      "[349.] [0.03920215]\n",
      "[349.] [137.05074]\n",
      "[791.] [0.18873864]\n",
      "[791.] [659.8303]\n",
      "[920.] [0.07589957]\n",
      "[920.] [265.3449]\n",
      "[1229.] [0.28568518]\n",
      "[1229.] [998.7554]\n",
      "[2776.] [0.05495491]\n",
      "[2776.] [192.12236]\n",
      "[1981.] [0.06720049]\n",
      "[1981.] [234.9329]\n",
      "[247.] [0.10319357]\n",
      "[247.] [360.7647]\n",
      "[850.] [0.20664978]\n",
      "[850.] [722.44763]\n",
      "[799.] [0.08024522]\n",
      "[799.] [280.5373]\n",
      "[510.] [0.26614487]\n",
      "[510.] [930.44244]\n",
      "[57.] [0.10756209]\n",
      "[57.] [376.03708]\n",
      "[278.] [0.04884308]\n",
      "[278.] [170.7554]\n",
      "[349.] [0.11430211]\n",
      "[349.] [399.6002]\n",
      "[1090.] [0.2235215]\n",
      "[1090.] [781.43115]\n",
      "[226.] [0.04306592]\n",
      "[226.] [150.55846]\n",
      "[222.] [0.0869297]\n",
      "[222.] [303.90625]\n",
      "[524.] [0.19793814]\n",
      "[524.] [691.99176]\n",
      "[141.] [0.05290589]\n",
      "[141.] [184.95898]\n",
      "[260.] [0.05771632]\n",
      "[260.] [201.77625]\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(y_test2[i],y_pred2[i])\n",
    "    print(y_test2[i],y_pred2[i]*MAXVIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
